# SPEED-OPTIMIZED CONFIGURATION FOR FASTEST MODEL RESPONSES
# Copy this to .env for maximum speed

# LLM Configuration - Groq with fastest model
LLM_PROVIDER=groq
MODEL_NAME=llama3-8b-8192  # Fastest Groq model with good quality

# Groq API Key (Get from https://console.groq.com)
GROQ_API_KEY=your_groq_api_key_here

# Alternative API Keys (optional)
TOGETHER_API_KEY=your_together_api_key
FIREWORKS_API_KEY=your_fireworks_api_key

# Database (optional, for persistence)
POSTGRES_URI=your_postgres_uri

# Embedding Model - Use smallest for fastest loading
EMBEDDING_MODEL=all-MiniLM-L6-v2  # Only 90MB, fast inference

# Pinecone Configuration
USE_CLOUD_RETRIEVER=true
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_environment
PINECONE_INDEX_NAME=document-reasoning
PINECONE_INDEX_HOST=your_pinecone_host
PINECONE_NAMESPACE=default

# Speed-Optimized Configuration
CHUNK_SIZE=800           # Smaller chunks = less context = faster responses
CHUNK_OVERLAP=100        # Minimal overlap for speed
TOP_K_RESULTS=3          # Fewer chunks = faster processing
VECTOR_DIMENSIONS=384    # Matches all-MiniLM-L6-v2 model

# Logging (reduce verbosity for speed)
LOG_LEVEL=WARNING
