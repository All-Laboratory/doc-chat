# OPTIMAL CONFIGURATION FOR DOCUMENT REASONING PROJECT
# Copy this content to your .env file

# =============================================================================
# LLM CONFIGURATION - Groq First, Together AI Fallback
# =============================================================================

# Primary LLM Provider: Groq (FAST)
GROQ_API_KEY=your_actual_groq_api_key_here  # GET FROM: https://console.groq.com
GROQ_MODEL=llama3-8b-8192

# Fallback LLM Provider: Together AI (RELIABLE)
TOGETHER_API_KEY=d681c57e9e5103697f72eccac56df183b76a1f15dbfbcaf39120dd298dbd7fd1
TOGETHER_MODEL=moonshotai/kimi-k2-instruct

# =============================================================================
# EMBEDDING MODEL - REQUIRED FOR SEMANTIC SEARCH
# =============================================================================

# Embedding model for document vectorization
EMBEDDING_MODEL=all-MiniLM-L6-v2
VECTOR_DIMENSIONS=384

# =============================================================================
# PINECONE CONFIGURATION - REQUIRED FOR VECTOR SEARCH
# =============================================================================

USE_CLOUD_RETRIEVER=true
PINECONE_API_KEY=your_pinecone_api_key_here  # GET FROM: https://pinecone.io
PINECONE_INDEX_NAME=document-reasoning
PINECONE_INDEX_HOST=https://document-reasoning-rfj2bhb.svc.aped-4627-b74a.pinecone.io
PINECONE_NAMESPACE=default

# =============================================================================
# OPTIMIZATION SETTINGS
# =============================================================================

# Document processing
CHUNK_SIZE=800
CHUNK_OVERLAP=100
TOP_K_RESULTS=3

# Performance
LOG_LEVEL=INFO

# Optional: Database (can work without)
# POSTGRES_URI=your_postgres_uri_here
